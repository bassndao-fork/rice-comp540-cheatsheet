\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{1 \ Linear models for regression}{1}{section*.1}}
\@writefile{toc}{\contentsline {subsection}{1.1 \ Least squares regression}{1}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{1.2 \ L2-Regularization: ridge regression}{1}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{1.3 \ L1-Regularization: lasso regression}{1}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{1.4 \ Locally weighted linear regression}{1}{section*.5}}
\@writefile{toc}{\contentsline {section}{2 \ Linear models for classification}{1}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{2.1 \ Discriminative models for classification}{1}{section*.7}}
\@writefile{toc}{\contentsline {subsection}{2.2 \ Generative models for classification}{1}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{2.2.1 \ Gaussian discriminant analysis (GDA)}{1}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{2.2.2 \ Naive Bayes models}{1}{section*.10}}
\@writefile{toc}{\contentsline {subsection}{2.3 \ Model criteria}{1}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{2.4 \ Multiclass classification}{1}{section*.12}}
\@writefile{toc}{\contentsline {subsubsection}{2.4.1 \ One vs. All (OVA) \& One vs. One (OVO)}{1}{section*.13}}
\@writefile{toc}{\contentsline {subsubsection}{2.4.2 \ Softmax}{1}{section*.14}}
\@writefile{toc}{\contentsline {section}{3 \ Kernel methods}{1}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{3.1 \ Kernel functions}{1}{section*.16}}
\@writefile{toc}{\contentsline {subsection}{3.2 \ Perceptron}{1}{section*.17}}
\@writefile{toc}{\contentsline {subsection}{3.3 \ Support vector machine (SVM)}{1}{section*.18}}
\@writefile{toc}{\contentsline {subsubsection}{3.3.1 \ Maximize margin}{1}{section*.19}}
\@writefile{toc}{\contentsline {subsubsection}{3.3.2 \ Non-separable case}{1}{section*.20}}
\@writefile{toc}{\contentsline {subsubsection}{3.3.3 \ Hinge loss -- \textit  {another view of SVM}}{1}{section*.21}}
\@writefile{toc}{\contentsline {subsubsection}{3.3.4 \ Multiclass SVM}{1}{section*.22}}
\@writefile{toc}{\contentsline {subsubsection}{3.3.5 \ SVM for regression}{1}{section*.23}}
\@writefile{toc}{\contentsline {section}{4 \ Neural Networks}{1}{section*.24}}
\@writefile{toc}{\contentsline {subsection}{4.1 \ Activation functions}{2}{section*.25}}
\@writefile{toc}{\contentsline {subsection}{4.2 \ Backpropagation}{2}{section*.26}}
\@writefile{toc}{\contentsline {subsection}{4.3 \ Convolutional neural networks}{2}{section*.27}}
\@writefile{toc}{\contentsline {subsection}{4.4 \ Optimization for training deep models}{2}{section*.28}}
\@writefile{toc}{\contentsline {subsection}{4.4.1 \ Momentum}{2}{section*.29}}
\@writefile{toc}{\contentsline {subsection}{4.4.2 \ Nesterov momentum}{2}{section*.30}}
\@writefile{toc}{\contentsline {subsection}{4.4.3 \ AdaGrad}{2}{section*.31}}
\@writefile{toc}{\contentsline {subsection}{4.4.4 \ RMSprop}{2}{section*.32}}
\@writefile{toc}{\contentsline {subsection}{4.4.5 \ Adam (adaptive moments)}{2}{section*.33}}
\@writefile{toc}{\contentsline {section}{5 \ Decision trees}{2}{section*.34}}
\@writefile{toc}{\contentsline {subsection}{5.1 \ Cost functions}{2}{section*.35}}
\@writefile{toc}{\contentsline {subsection}{5.2 \ Decision tree for regression}{2}{section*.36}}
\@writefile{toc}{\contentsline {subsection}{5.3 \ Node is not worth splitting when}{2}{section*.37}}
\@writefile{toc}{\contentsline {subsection}{5.4 \ Avoid overfitting}{2}{section*.38}}
\@writefile{toc}{\contentsline {section}{6 \ Ensembles}{2}{section*.39}}
\@writefile{toc}{\contentsline {subsection}{6.1 \ Bagging}{2}{section*.40}}
\@writefile{toc}{\contentsline {subsection}{6.2 \ Boosting}{2}{section*.41}}
\@writefile{toc}{\contentsline {subsection}{6.3 \ Gradient boosting}{2}{section*.42}}
\@writefile{toc}{\contentsline {subsubsection}{6.3.1 \ Gradient boosting for regression}{2}{section*.43}}
\@writefile{toc}{\contentsline {subsubsection}{6.3.1 \ Gradient boosting for classification}{2}{section*.44}}
\@writefile{toc}{\contentsline {section}{7 \ Probabilistic graphical models}{2}{section*.45}}
\@writefile{toc}{\contentsline {subsection}{7.1 \ Directed models -- Bayesian network}{2}{section*.46}}
\@writefile{toc}{\contentsline {subsection}{7.2 \ Undirected models -- Markov network}{2}{section*.47}}
\@writefile{toc}{\contentsline {subsection}{7.3 \ Sampling}{3}{section*.48}}
\@writefile{toc}{\contentsline {subsubsection}{7.3.1 \ Ancestral sampling}{3}{section*.49}}
\@writefile{toc}{\contentsline {subsubsection}{7.3.2 \ Gibbs sampling}{3}{section*.50}}
\@writefile{toc}{\contentsline {subsection}{7.4 \ Hidden Markov models}{3}{section*.51}}
\@writefile{toc}{\contentsline {subsubsection}{7.4.1 \ Forward computation -- filtering}{3}{section*.52}}
\@writefile{toc}{\contentsline {subsubsection}{7.4.2 \ Backward computation -- smoothing}{3}{section*.53}}
\@writefile{toc}{\contentsline {subsubsection}{7.4.3 \ Viterbi algorithm -- most likely sequence}{3}{section*.54}}
\@writefile{toc}{\contentsline {subsubsection}{7.4.4 \ Parameters estimation}{3}{section*.55}}
\@writefile{toc}{\contentsline {section}{8 \ Unsupervised learning}{3}{section*.56}}
\@writefile{toc}{\contentsline {subsection}{8.1 \ Principal components analysis (PCA)}{3}{section*.57}}
\@writefile{toc}{\contentsline {subsubsection}{8.1.1 \ PCA method}{3}{section*.58}}
\@writefile{toc}{\contentsline {subsubsection}{8.1.2 \ Kernel PCA}{3}{section*.59}}
\@writefile{toc}{\contentsline {subsection}{8.2 \ Expectation maximization algorithm (EM)}{3}{section*.60}}
\@writefile{toc}{\contentsline {subsubsection}{8.2.1 \ K-means}{3}{section*.61}}
\@writefile{toc}{\contentsline {subsubsection}{8.2.2 \ Gaussian mixture model (GMM)}{3}{section*.62}}
\@writefile{toc}{\contentsline {section}{9 \ Reinforcement learning}{3}{section*.63}}
\@writefile{toc}{\contentsline {subsection}{9.1 \ Markov decision process (MDP)}{3}{section*.64}}
\@writefile{toc}{\contentsline {subsection}{9.2 \ Model-based RL}{3}{section*.65}}
\@writefile{toc}{\contentsline {subsection}{9.3 \ Model-free RL}{3}{section*.66}}
\@writefile{toc}{\contentsline {subsection}{9.4 \ TD learning}{3}{section*.67}}
\@writefile{toc}{\contentsline {subsection}{9.5 \ Q learning}{3}{section*.68}}
\@writefile{toc}{\contentsline {section}{Appendix}{3}{section*.69}}
\@writefile{toc}{\contentsline {subsection}{Distributions}{3}{section*.70}}
\@writefile{toc}{\contentsline {subsection}{Information theory}{3}{section*.71}}
\@writefile{toc}{\contentsline {subsection}{Convex function}{4}{section*.72}}
\@writefile{toc}{\contentsline {subsection}{Convergence of perceptron}{4}{section*.73}}
\@writefile{toc}{\contentsline {subsection}{Mercer's theorem}{4}{section*.74}}
\@writefile{toc}{\contentsline {subsection}{Adaboost principle}{4}{section*.75}}
\@writefile{toc}{\contentsline {subsection}{Kernel PCA derivation}{4}{section*.76}}
\@writefile{toc}{\contentsline {subsection}{Correctness of EM}{4}{section*.77}}
